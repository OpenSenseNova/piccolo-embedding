<small>[EN](README.md) | [ç®€ä½“ä¸­æ–‡](README_zh.md) </small>

# [Piccolo2: General Text Embeddings with Multi-Task Hybrid loss Training](https://arxiv.org/abs/2405.06932)

ğŸš€ **New SOTA on CMTEB** 

ğŸ”¥ æˆ‘ä»¬æœ€æ–°çš„é€šç”¨embeddingæ¨¡å‹ [sensenova/piccolo-large-zh-v2](https://huggingface.co/sensenova/piccolo-large-zh-v2) åœ¨CMTEBè¯„æµ‹æ¦œå•ä¸Šå–å¾—äº†70.95çš„å‡åˆ†! [2024/4/23]

<details>
<summary>ğŸ“„ CMTEBç»“æœ [ç‚¹å‡»å±•å¼€]</summary>
<p align='center'>
<img src='assets/cmteb-0505.png'>
</p>
</details>

## ğŸ’¡Model Highlights
Piccolo2 åœ¨CMTEBæ¦œå•ä¸Šçš„6é¡¹ä»»åŠ¡çš„ç»¼åˆè¯„ä¼°ä¸­è¶…è¶Šäº†å…¶ä»–æ¨¡å‹ï¼Œç›®å‰ä½äºç¬¬ä¸€ä½ã€‚Piccolo2 ä¸»è¦åˆ©ç”¨é«˜æ•ˆçš„å¤šä»»åŠ¡æ··åˆæŸå¤±è®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨æ¥è‡ªä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„æ–‡æœ¬æ•°æ®å’Œæ ‡ç­¾ã€‚ æ­¤å¤–ï¼ŒPiccolo2 æ‰©å¤§äº†Embeddingç»´åº¦ï¼Œå¹¶ä½¿ç”¨MRLè®­ç»ƒæ¥æ”¯æŒæ›´çµæ´»çš„å‘é‡ç»´åº¦ã€‚
huggingfaceä¸Šæ”¾äº†æˆ‘ä»¬æœ€æ–°çš„æ¨¡å‹: https://huggingface.co/sensenova  
å¯¹äºè®­ç»ƒç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒæˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Š: https://arxiv.org/abs/2405.06932

## ğŸ“– Repo Details
åœ¨è¿™ä¸ªrepoé‡Œé¢ï¼Œæˆ‘ä»¬æ”¾å‡ºäº†è®­ç»ƒçš„ä»£ç ï¼Œé‡Œé¢æä¾›äº†ä¸€äº›æœ‰åŠ©äºæå‡Embeddingæ¨¡å‹æ€§èƒ½çš„è®­ç»ƒæŠ€å·§ï¼š
- Multi-task Hybrid Loss Training
- Matryoshka Representation Learning
- Embdding Dimension Scaling
- Task-Homogenous Dataset
- Position Embedding Hierarchical Decomposition 

ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œæˆ‘ä»¬é»˜è®¤ä½¿ç”¨ deepspeed-zero1ã€gradient checkpointingå’Œmix-precisionè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªè„šæœ¬ç”¨æ¥å¸®åŠ©å¤§å®¶è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚

### Tips
1. è¯¥é¡¹ç›®ä¼šé»˜è®¤ä½¿ç”¨multi-task hybrid lossæ¥è¿›è¡Œè®­ç»ƒï¼Œå‰ææ˜¯æ•°æ®æŒ‰è§„å®šæ ¼å¼å‡†å¤‡å¥½ã€‚

2. å¯¹äºscaling dimension length, æˆ‘ä»¬å°†å®ƒçš„ä¸€äº›å‚æ•°å†™æ­»åœ¨äº†ä»£ç é‡Œé¢, è¯·æ ¹æ®éœ€è¦è‡ªè¡Œæ›´æ”¹:
```python
self.scaling_layer = ScalingLayer(origin_dim=1024, scaling_dim=1792)
if os.path.exists(os.path.join(model_name_or_path, '2_Dense/pytorch_model.bin')):
    scaling_layer_state_dict = torch.load(os.path.join(model_name_or_path, '2_Dense/pytorch_model.bin'))
    self.scaling_layer.load_state_dict(scaling_layer_state_dict, strict=True)
```
3. å¯¹äºMRLè®­ç»ƒ, æˆ‘ä»¬ä¹ŸæŠŠå®ƒçš„å‚æ•°å†™æ­»åœ¨äº†ä»£ç é‡Œ.
```python
self.mrl_nesting_list = [256, 512, 768, 1024, 1280, 1536, 1792]
```

4. å¦‚æœä½ æƒ³å¢é•¿æ¨¡å‹çš„position embeddingï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç®€å•çš„å±‚æ¬¡åˆ†è§£æ–¹æ³•, åªéœ€è¦å°† `extend_pe` è®¾ç½®ä¸ºTrue ç„¶åæŠŠ `max_length` è®¾ç½®ä¸ºä½ çš„ç›®æ ‡é•¿åº¦å°±å¯ä»¥äº†.


## ğŸ”¨ ä½¿ç”¨æŒ‡å—
### 1. ç¯å¢ƒ
```shell
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡
æˆ‘ä»¬å°†æ•°æ®é›†åˆ†ä¸ºä¸‰å¤§ç±»ï¼šæ£€ç´¢/æ’åºã€èšç±»/åˆ†ç±»ã€å¥å¯¹ç›¸ä¼¼åº¦/å¥å¯¹åˆ†ç±»ï¼Œå¹¶å¯¹ä¸åŒç±»åˆ«é‡‡ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚ åœ¨`data_example`ç›®å½•ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†è¿™ä¸‰ç§ç±»å‹æ•°æ®çš„ç¤ºä¾‹ã€‚

1) `Retri`: å¯¹äºæ£€ç´¢ã€é‡æ’æ•°æ®é›†, æˆ‘ä»¬é‡‡ç”¨æ ‡å‡†çš„InfoNCEè¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶é‡‡ç”¨in-batch-negativeæ¥æ‰©å……è´Ÿæ ·æœ¬æ•°é‡ã€‚è¿™ä¸ªæ•°æ®é›†éœ€è¦å››åˆ—: `text`, `text_pos`, `text_neg`, `type`. è¿™é‡Œçš„ 'type' éœ€è¦è¢«æ ‡æ³¨ä¸º 'retri_contrast'ã€‚

2) `STS`: å¯¹äºå¥å¯¹ç›¸ä¼¼åº¦ï¼Œå¥å¯¹åˆ†ç±»æ•°æ®é›†, æˆ‘ä»¬é‡‡ç”¨äº†cosentè¿™ç§æ’åºæŸå¤±, è¿™ä¸ªæ•°æ®é›†åŒæ ·æœ‰å››åˆ—: `text`, `text_pair`, `label`, `type`. è¿™é‡Œ 'type' éœ€è¦è¢«æ ‡æ³¨ä¸º 'cosent'ã€‚
   
3) `Cls`: å¯¹äºåˆ†ç±»ã€èšç±»ä»»åŠ¡, æˆ‘ä»¬å°†æ–‡æœ¬å’Œå®ƒçš„è¯­ä¹‰æ ‡ç­¾è§†ä¸ºæ­£è´Ÿæ ·æœ¬å¯¹ï¼ŒåŒæ ·é‡‡ç”¨äº†InfoNCEæŸå¤±æ¥ä¼˜åŒ–ï¼Œä½†ä¸å†é‡‡æ ·in-batch-negativeï¼ˆå› ä¸ºè¿™å¾ˆå®¹æ˜“é€ æˆè®­ç»ƒå†²çªï¼‰,è¿™ç±»æ•°æ®é›†åŒæ ·åŒ…å«å››åˆ—: `text`, `text_pos`, `text_neg`, `type`. è¿™é‡Œ 'type' éœ€è¦è¢«æ ‡æ³¨ä¸º 'cls_contrast'

'type' åˆ—è¡¨æ˜äº†å½“å‰æ•°æ®çš„ç±»å‹ï¼Œåœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬é€šè¿‡è·å–å½“å‰æ•°æ®çš„ç±»å‹ï¼Œä»¥é‡‡ç”¨ä¸åŒçš„æŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚

### 3. è®­ç»ƒ
æˆ‘ä»¬æä¾›äº†è®­ç»ƒçš„è„šæœ¬ `scripts/ft.sh`. ä¸‹é¢æˆ‘ä»¬å¯¹è¿™ä¸ªè„šæœ¬é‡Œçš„ä¸€äº›å˜é‡åšäº†ç®€å•çš„è§£é‡Š.

**ç¯å¢ƒå‚æ•°**  
- ROOT: ä¸ºè¯¥é¡¹ç›®åœ¨æœ¬åœ°æœºå™¨ä¸Šçš„ç»å¯¹è·¯å¾„. 
- GPUS_PER_NODE: å•å¡çš„GPUæ•°é‡
é»˜è®¤æä¾›çš„è„šæœ¬æ˜¯åœ¨å•æœºä¸‹è¿›è¡Œè®­ç»ƒçš„ï¼Œå¦‚æœä½ æ˜¯åœ¨å¤šæœºå¤šå¡ä¸‹è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œä½ éœ€è¦é¢å¤–å¡«ä¸Š:
- WORLD_SIZE: æœºå™¨çš„æ•°é‡
- RANK: å½“å‰æœºå™¨çš„Rankåºå·ï¼Œé€šå¸¸éœ€è¦ä»SLURMçš„ç¯å¢ƒå˜é‡è·å–
- MASTER_ADDR:MASTER_PORT: é€šä¿¡ç«¯å£


**è®­ç»ƒå‚æ•°** 
- MODEL_NAME_OR_PATH: pretrain modelçš„ç»å¯¹è·¯å¾„ã€‚
- DS_PATH: deepspeedå‚æ•°, é»˜è®¤çš„configæ”¾åœ¨äº† `./de_config_zero1.json`ã€‚
- META_PATHS: ä½¿ç”¨çš„æ•°æ®é›†åˆ—è¡¨. æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ ·æœ¬ `meta_lists/piccolo.txt`. è¯¥txtæ–‡ä»¶çš„æ¯ä¸€è¡Œæœ‰ä¸¤åˆ—ï¼Œç¬¬ä¸€åˆ—æ˜¯æ•°æ®é›†çš„ç›¸å¯¹è·¯å¾„ï¼Œç¬¬äºŒåˆ—æ˜¯æ•°æ®é›†çš„repeatæ¬¡æ•°ã€‚
- ROOT_DIRS: æ•°æ®é›†çš„ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚

**Run**
```shell
bash scripts/ft.sh
```

## ğŸ¤— **Model List**
| Model|è¯­è¨€||ç®€ä»‹|prompt|
|:-|:-:|:-:|:--------------------------------------------:|:---------:|
| [sensenova/piccolo-large-zh-v2](https://huggingface.co/sensenova/piccolo-large-zh-v2)                   |    Chinese     |   | version2: é‡‡ç”¨äº†å¤šä»»åŠ¡æ··åˆæŸå¤±è¿›è¡Œè®­ç»ƒ | None |
| [sensenova/piccolo-large-zh](https://huggingface.co/sensenova/piccolo-large-zh)                   |    Chinese     |   | version1: ä½¿ç”¨4äº¿çš„ä¸­æ–‡pairå¯¹è¿›è¡Œé¢„è®­ç»ƒ | 'æŸ¥è¯¢'/'ç»“æœ' |
| [sensenova/piccolo-base-zh](https://huggingface.co/sensenova/piccolo-base-zh)                   |    Chinese     |   | version1: ä½¿ç”¨4äº¿çš„ä¸­æ–‡pairå¯¹è¿›è¡Œé¢„è®­ç»ƒ | 'æŸ¥è¯¢'/'ç»“æœ' |


## Citation
å¦‚æœæ‚¨è®¤ä¸ºæˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šã€æ¨¡å‹ã€ä»£ç æœ‰å¸®åŠ©ï¼Œè¯·åƒä¸‹é¢è¿™æ ·å¼•ç”¨æˆ‘ä»¬çš„æ–‡ç« ï¼Œæˆ–è€…åœ¨githubå’Œhuggingfaceä¸Šç‚¹ä¸€ä¸ªå…è´¹çš„èµï¼
```bibtex
@misc{2405.06932,
Author = {Junqin Huang and Zhongjie Hu and Zihao Jing and Mengya Gao and Yichao Wu},
Title = {Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training},
Year = {2024},
Eprint = {arXiv:2405.06932},
}
```